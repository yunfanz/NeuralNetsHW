{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In this notebook, you will implement a deep Q-Learning reinforcement algorithm. The implementation borrows ideas from both the original DeepMind Nature paper and the more recent asynchronous version:<br/>\n",
    "[1] \"Human-Level Control through Deep Reinforcement Learning\" by Mnih et al. 2015<br/>\n",
    "[2] \"Asynchronous Methods for Deep Reinforcement Learning\" by Mnih et al. 2016.<br/>\n",
    "\n",
    "In particular:\n",
    "* We use separate target and Q-functions estimators with periodic updates to the target estimator. \n",
    "* We use several concurrent \"threads\" rather than experience replay to generate less biased gradient updates. \n",
    "* Threads are actually synchronized so we start each one at a random number of moves.\n",
    "* We use an epsilon-greedy policy that blends random moves with policy moves.\n",
    "* We taper the random action parameter (epsilon) and the learning rate to zero during training.\n",
    "\n",
    "This gives a simple and reasonably fast general-purpose RL algorithm. We use it here for the Cartpole environment from OpenAI Gym, but it can easily be adapted to others. For this notebook, you will implement 4 steps:\n",
    "\n",
    "1. The backward step for the Q-estimator\n",
    "2. The $\\epsilon$-greedy policy\n",
    "3. \"asynchronous\" initialization \n",
    "4. The Q-learning algorithm\n",
    "\n",
    "To get started, we import some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below lists some parameters you can tune. They should be self-explanatory. They are currently set to train CartPole-V0 to a \"solved\" score (> 195) most of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsteps = 10001                       # Number of steps to run (game actions per environment)\n",
    "npar = 16                            # Number of parallel environments\n",
    "target_window = 200                  # Interval to update target estimator from q-estimator\n",
    "discount_factor = 0.99               # Reward discount factor\n",
    "printsteps = 1000                    # Number of steps between printouts\n",
    "render = False                       # Whether to render an environment while training\n",
    "\n",
    "epsilon_start = 1.0                  # Parameters for epsilon-greedy policy: initial epsilon\n",
    "epsilon_end = 0.0                    # Final epsilon\n",
    "neps = int(0.5*nsteps)               # Number of steps to decay epsilon\n",
    "\n",
    "learning_rate = 2e-3                 # Initial learning rate\n",
    "lr_end = 0                           # Final learning rate\n",
    "nlr = neps                           # Steps to decay learning rate\n",
    "decay_rate = 0.99                    # Decay factor for RMSProp \n",
    "\n",
    "nhidden = 200                        # Number of hidden layers for estimators\n",
    "\n",
    "init_moves = 2000                    # Upper bound on random number of moves to take initially\n",
    "nwindow = 2                          # Sensing window = last n images in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are environment-specific parameters. The function \"preprocess\" should process an observation returned by the environment into a vector for training. For CartPole we simply append a 1 to implement bias in the first layer. \n",
    "\n",
    "For visual environments you would typically crop, downsample to 80x80, set color to a single bit (foreground/background), and flatten to a vector. That transformation is already implemented in the Policy Gradient code.\n",
    "\n",
    "*nfeats* is the dimension of the vector output by *preprocess*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game_type=\"CartPole-v0\"                 # Model type and action definitions\n",
    "VALID_ACTIONS = [0, 1]\n",
    "nactions = len(VALID_ACTIONS)\n",
    "nfeats = 5                              # There are four state features plus the constant we add\n",
    "\n",
    "def preprocess(I):                      # preprocess each observation\n",
    "    \"\"\"Just append a 1 to the end\"\"\"\n",
    "    return np.append(I.astype(float),1) # Add a constant feature for bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Q-estimator class. We use two instances of this class, one for the target estimator, and one for the Q-estimator. The Q function is normally represented as a scalar $Q(x,a)$ where $x$ is the state and $a$ is an action. For ease of implementation, we actually estimate a vector-valued function $Q(x,.)$ which returns the estimated reward for every action. The model here has just a single hidden layer:\n",
    "\n",
    "<pre>\n",
    "Input Layer (nfeats) => FC Layer => RELU => FC Layer => Output (naction values)\n",
    "</pre>\n",
    "\n",
    "## 1. Implement Q-estimator gradient\n",
    "Your first task is to implement the\n",
    "<pre>Estimator.gradient(s, a, y)</pre>\n",
    "method for this class. **gradient** should compute the gradients wrt weight arrays W1 and W2 into\n",
    "<pre>self.grad['W1']\n",
    "self.grad['W2']</pre>\n",
    "respectively. Both <code>a</code> and <code>y</code> are vectors. Be sure to update only the output layer weights corresponding to the given action vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "\n",
    "    def __init__(self, ninputs, nhidden, nactions):\n",
    "        \"\"\" Create model matrices, and gradient and squared gradient buffers\"\"\"\n",
    "        model = {}\n",
    "        model['W1'] = np.random.randn(nhidden, ninputs) / np.sqrt(ninputs)   # \"Xavier\" initialization\n",
    "        model['W2'] = np.random.randn(nactions, nhidden) / np.sqrt(nhidden)\n",
    "        self.model = model\n",
    "        self.grad = { k : np.zeros_like(v) for k,v in model.iteritems() }\n",
    "        self.gradsq = { k : np.zeros_like(v) for k,v in model.iteritems() }   \n",
    "        \n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\" Run the model forward given a state as input.\n",
    "    returns action predictions and the hidden state\"\"\"\n",
    "        h = np.dot(self.model['W1'], s)\n",
    "        h[h<0] = 0 # ReLU nonlinearity\n",
    "        rew = np.dot(self.model['W2'], h)\n",
    "        return rew, h\n",
    "    \n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\" Predict the action rewards from a given input state\"\"\"\n",
    "        rew, h = self.forward(s)\n",
    "        return rew\n",
    "    \n",
    "              \n",
    "    def gradient(self, s, a, y):\n",
    "        \"\"\" Given a state s, action a and target y, compute the model gradients\"\"\"\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Compute gradients and return a scalar loss on a minibatch of size npar ##\n",
    "    ##    s is the input state matrix (ninputs x npar).                             ##\n",
    "    ##    a is an action vector (npar,).                                            ##\n",
    "    ##    y is a vector of target values (npar,) corresponding to those actions.    ##\n",
    "    ##    return: the loss per sample (npar,).                                      ##                          \n",
    "    ##                                                                              ##\n",
    "    ## Notes:                                                                       ##\n",
    "    ##    * If the action is ai in [0,...,nactions-1], backprop only through the    ##\n",
    "    ##      ai'th output.                                                           ##\n",
    "    ##    * loss should be L2, and we recommend you normalize it to a per-input     ##\n",
    "    ##      value, i.e. return L2(target,predition)/sqrt(npar).                     ##\n",
    "    ##    * save the gradients in self.grad['W1'] and self.grad['W2'].              ##\n",
    "    ##    * update self.grad['W1'] and self.grad['W2'] by adding the gradients, so  ##\n",
    "    ##      that multiple gradient steps can be used beteween updates.              ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "        loss = 0.0\n",
    "        rew, h = self.forward(s)\n",
    "        Q = rew[a, np.arange(a.size)]\n",
    "        loss = np.sum((Q-y)**2)/np.sqrt(npar)/np.sqrt(npar)\n",
    "        dQ = (Q-y).reshape((1,-1))*2./np.sqrt(npar)/np.sqrt(npar)  #!!!sign\n",
    "        mask = np.where(h>0, np.ones_like(h), np.zeros_like(h))\n",
    "#         self.grad['W2'][a] += (h*dQ).T\n",
    "        for i, A in enumerate(a):\n",
    "            self.grad['W2'][A] += (h*dQ).T[i]\n",
    "        gradh = (dQ*self.model['W2'][a].T*mask)#.reshape((-1,1))\n",
    "        self.grad['W1'] += np.dot(gradh,s.T)\n",
    "        return loss\n",
    "    \n",
    "    def ex_gradient(self, s, a, y):\n",
    "        \"\"\" for numerical gradient check\"\"\"\n",
    "        grads = { k : np.zeros_like(v) for k,v in self.model.iteritems() }\n",
    "        loss = 0.0\n",
    "        rew, h = self.forward(s)\n",
    "        Q = rew[a, np.arange(a.size)]\n",
    "        loss = np.sum((Q-y)**2)/np.sqrt(npar)\n",
    "        dQ = -(y-Q).reshape((1,-1))*2./np.sqrt(npar)  #!!!sign\n",
    "        mask = np.where(h>0, np.ones_like(h), np.zeros_like(h))\n",
    "        for i, A in enumerate(a):\n",
    "            grads['W2'][A] += (h*dQ).T[i]\n",
    "        gradh = (dQ*self.model['W2'][a].T*mask)#.reshape((-1,1))\n",
    "        grads['W1'] += np.dot(gradh,s.T)\n",
    "        return grads, loss\n",
    "    \n",
    "    \n",
    "    def rmsprop(self, learning_rate, decay_rate): \n",
    "        \"\"\" Perform model updates from the gradients using RMSprop\"\"\"\n",
    "        for k in self.model:\n",
    "            g = self.grad[k]\n",
    "            self.gradsq[k] = decay_rate * self.gradsq[k] + (1 - decay_rate) * g*g\n",
    "            self.model[k] -= learning_rate * g / (np.sqrt(self.gradsq[k]) + 1e-5)\n",
    "            self.grad[k].fill(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement $\\epsilon$-Greedy Policy\n",
    "\n",
    "An $\\epsilon$-Greedy policy should:\n",
    "* with probability $\\epsilon$ take a uniformly-random action.\n",
    "* otherwise choose the best action according to the estimator from the given state.\n",
    "\n",
    "The function below should implement this policy. It should return a matrix A of size (nactions, npar) such that A[i,j] is the probability of taking action i on input j. The probabilities of non-optimal actions should be $\\epsilon/{\\rm nactions}$ and the probability of the best action should be $1-\\epsilon+\\epsilon/{\\rm nactions}$.\n",
    "\n",
    "Since the function processes batches of states, the input <code>state</code> is a <code>ninputs x npar</code> matrix, and the returned value should be a <code>nactions x npar</code> matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy(estimator, state, epsilon):\n",
    "    \"\"\" Take an estimator and state and predict the best action.\n",
    "    For each input state, return a vector of action probabilities according to an epsilon-greedy policy\"\"\"\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Implement an epsilon-greedy policy                                     ##\n",
    "    ##       estimator: is the estimator to use (instance of Estimator)             ##\n",
    "    ##       state is an (ninputs x npar) state matrix                              ##\n",
    "    ##       epsilon is the scalar policy parameter                                 ##\n",
    "    ## return: an (nactions x npar) matrix A where A[i,j] is the probability of     ##\n",
    "    ##       taking action i on input j.                                            ##\n",
    "    ##                                                                              ##\n",
    "    ## Use the definition of epsilon-greedy from the cell above.                    ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "\n",
    "    A = np.zeros((nactions, npar))\n",
    "    A += epsilon/nactions\n",
    "    actions = np.argmax(estimator.predict(state), axis=0) #orig naction by npar\n",
    "    for i, act in enumerate(actions):\n",
    "        A[act,i] += 1-epsilon\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine copies the state of one estimator into another. Its used to update the target estimator from the Q-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_estimator(to_estimator, from_estimator, window, istep):\n",
    "    \"\"\" every <window> steps, Copy model state from from_estimator into to_estimator\"\"\"\n",
    "    if (istep % window == 0):\n",
    "        for k in from_estimator.model:\n",
    "            np.copyto(to_estimator.model[k], from_estimator.model[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement \"Asynchronous Threads\"\n",
    "\n",
    "Don't try that in Python!! Actually all we do here is create an array of environments and advance each one a random number of steps, using random actions at each step. Later on we will make *synchronous* updates to all the environments, but the environments (and their gradient updates) should remain uncorrelated. This serves the same goal as asynchronous updates in paper [2], or experience replay in paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 12:31:29,200] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,223] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,267] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,290] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,321] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,353] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,363] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,383] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,417] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,450] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,475] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,499] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,519] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,549] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,589] Making new env: CartPole-v0\n",
      "[2016-11-22 12:31:29,617] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "block_reward = 0.0;\n",
    "total_epochs = 0;\n",
    "   \n",
    "# Create estimators\n",
    "q_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "target_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "\n",
    "# The epsilon and learning rate decay schedules\n",
    "epsilons = np.linspace(epsilon_start, epsilon_end, neps)\n",
    "learning_rates = np.linspace(learning_rate, lr_end, nlr)\n",
    "\n",
    "# Initialize the games\n",
    "print(\"Initializing games...\"); sys.stdout.flush()\n",
    "envs = np.empty(npar, dtype=object)\n",
    "state = np.zeros([nfeats * nwindow, npar], dtype=float)\n",
    "new_state = np.zeros([nfeats * nwindow, npar], dtype=float)\n",
    "rewards = np.zeros([npar], dtype=float)\n",
    "dones = np.empty(npar, dtype=int)\n",
    "actions = np.zeros([npar], dtype=int)\n",
    "\n",
    "\n",
    "for i in range(npar):\n",
    "    envs[i] = gym.make(game_type)\n",
    "    n_step = np.random.randint(nwindow, init_moves)\n",
    "    for j in xrange(n_step):\n",
    "        action = np.random.choice(VALID_ACTIONS)\n",
    "        observation, reward, done, info = envs[i].step(action)\n",
    "        rewards[i] = reward\n",
    "        state[:,i] = np.concatenate((state[nfeats:,i], preprocess(observation)))\n",
    "        block_reward += reward\n",
    "        if done:\n",
    "            observation = envs[i].reset()\n",
    "            #state[:,i] = np.concatenate((np.zeros_like(state[nfeats:,i]), preprocess(observation)))\n",
    "            total_epochs += 1\n",
    "            #rewards[i] = 0\n",
    "            \n",
    "    \n",
    "   \n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Advance each environment by a random number of steps, where the number ##\n",
    "    ##       of steps is sampled uniformly from [nwindow, init_moves].              ##\n",
    "    ##       Use random steps to advance.                                           ## \n",
    "    ##                                                                              ##\n",
    "    ## Update the total reward and total epochs variables as you go.                ##\n",
    "    ## If an environment returns done=True, reset it and increment the epoch count. ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Deep Q-Learning\n",
    "In this cell you actually implement the algorithm. We've given you comments to define all the steps. You should also add book-keeping steps to keep track of the loss, reward and number of epochs (where env.step() returns done = true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.01709302657 9.01709302657\n",
      "W2 max relative error: 2.854975e-09\n",
      "W1 max relative error: 7.591903e-07\n"
     ]
    }
   ],
   "source": [
    "from cs294_129.gradient_check import eval_numerical_gradient\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "ss, aa, yy = np.array([state[:,0]]).T, np.array([actions[0]]), np.array([y[0]])\n",
    "ss, aa, yy = state, actions, y\n",
    "grads, loss = q_estimator.ex_gradient(ss, aa, yy)\n",
    "print loss, q_estimator.gradient(ss, aa, yy)\n",
    "for param_name in grads:\n",
    "    f = lambda W: q_estimator.ex_gradient(ss, aa, yy)[1]\n",
    "    param_grad_num = eval_numerical_gradient(f, q_estimator.model[param_name], verbose=False)\n",
    "#     print param_grad_num\n",
    "#     print grads[param_name]\n",
    "    #print '%s max relative mag: %e' % (param_name, rel_error(param_grad_num, np.zeros_like(param_grad_num)))\n",
    "    print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 False\n",
      "1.0 False\n",
      "1.0 False\n",
      "1.0 False\n",
      "1.0 False\n",
      "1.0 False\n",
      "1.0 False\n",
      "1.0 False\n",
      "1.0 False\n",
      "1.0 False\n",
      "1.0 True\n",
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.83694104609541"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env=envs[0]\n",
    "i=0\n",
    "done = False\n",
    "_ = env.reset()\n",
    "while not done and i < 100:\n",
    "    i+=1\n",
    "    observation, reward, done, info = env.step(np.random.choice(VALID_ACTIONS))\n",
    "    print reward, done\n",
    "print i\n",
    "(1-0.99**22)/0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.54594431  1.16689433  1.67661509  1.2622613   1.88609791  1.81639218\n",
      "   1.42641751  1.18032009  1.43651016  1.54633194  1.32394912  1.43129812\n",
      "   1.42454173  1.12972263  1.35243752  2.0213679 ]\n",
      " [ 1.3613525   1.37561319  2.26482377  1.46564378  1.62645232  2.4612569\n",
      "   1.65580744  1.24541686  1.6747149   1.38252453  1.8532417   1.30605075\n",
      "   1.67088427  1.11986066  1.55314033  1.79320141]]\n",
      "step 0, time 0.0, loss 0.00161422, epochs 789, reward/epoch 22.11534\n",
      "[[ 5.25798678  1.16317915  5.40350258  5.38377692  4.98016939  5.42909232\n",
      "   4.6861084   5.42527161  4.34947034  5.36038535  5.7399534   5.24423642\n",
      "   5.20082847  2.49900524  4.01998496  5.10963461]\n",
      " [ 5.4729992   1.73746687  6.05160528  5.41465633  5.50392263  5.79290865\n",
      "   4.71174616  5.78427058  4.37827431  5.52516206  5.9402688   5.69059978\n",
      "   5.65718508  2.19316804  3.89025131  5.59140373]]\n",
      "step 1000, time 1.0, loss 0.16363658, epochs 1579, reward/epoch 20.25316\n",
      "[[ 10.18680028  10.61824687   6.93419374   9.20017044  10.06042467\n",
      "    9.62393583  10.01273577   3.87871106  10.02775226   9.88981572\n",
      "    5.32787981   5.95617372  10.18688936   6.72802561   5.02008833\n",
      "   10.19329563]\n",
      " [ 10.16222433  10.71172407   6.52425477   9.16139531  10.22466745\n",
      "    9.49231047  10.18009003   3.1931334   10.21977381  10.2102702\n",
      "    5.90638932   5.41986449  10.2773424    6.22292986   5.66302474\n",
      "   10.35102811]]\n",
      "step 2000, time 1.9, loss 0.47903617, epochs 2535, reward/epoch 16.73640\n",
      "[[ 14.53722616  14.18100388  15.2249873   11.7374098   14.43469501\n",
      "    8.18908845  10.56611333  14.73212661  14.60237716  14.27170254\n",
      "   11.32891883  14.85953802  14.74126778  14.58854933  14.943818\n",
      "   15.07752492]\n",
      " [ 14.41260502  13.95216227  15.22251667  11.28902483  14.21530686\n",
      "    7.55373371  10.04595178  14.62334358  14.53135551  14.15097346\n",
      "   11.50985808  14.81857684  14.56408738  14.59752324  14.74978051\n",
      "   14.87351434]]\n",
      "step 3000, time 2.9, loss 0.50098210, epochs 3545, reward/epoch 15.84158\n",
      "[[ 19.02523019  19.4318157   15.91622345  18.91021923   7.79327849\n",
      "   18.9205681   19.15032291  19.2741968   19.21042446  19.44760445\n",
      "   19.21560361  19.42792143  16.92199104  18.43912045  19.55385369\n",
      "   19.18926325]\n",
      " [ 19.00943585  19.21434296  15.63351222  18.94734306   7.21869375\n",
      "   18.8818708   19.08468882  19.19155058  19.31497923  19.34918714\n",
      "   19.3083075   19.1404767   16.49089464  18.3039558   19.4549358\n",
      "   19.258801  ]]\n",
      "step 4000, time 3.9, loss 0.60387821, epochs 4798, reward/epoch 12.76935\n",
      "[[ 21.38964283   1.39231868  17.13931397  22.14805947  21.76089703\n",
      "   11.25671343  22.31498453  24.26195879  22.311832     1.11627928\n",
      "   13.60770987   8.20539213  22.3459184    2.8354759   22.39025235\n",
      "   22.64111744]\n",
      " [ 22.57321001   2.56760742  18.38129494  23.19703524  22.90714679\n",
      "   12.50648547  23.36966672  25.13540484  23.39769932   2.29123552\n",
      "   14.80500687   9.42596431  23.40620112   4.01200821  23.46961363\n",
      "   23.72097354]]\n",
      "step 5000, time 5.0, loss 0.91844333, epochs 6394, reward/epoch 10.02506\n",
      "[[ 19.07933297  18.48176328  12.41009557  18.02018904  13.54235933\n",
      "    4.70026995  23.37356417  22.44844776   0.38641977  22.60816904\n",
      "    8.18100639  20.64930267  23.47099905  10.63328357  24.71191261\n",
      "   22.36467528]\n",
      " [ 20.32832714  19.72059683  13.65738599  19.27173507  14.78278818\n",
      "    5.89134338  24.27055277  23.54923793   1.61155764  23.73007519\n",
      "    9.37755789  21.92492511  24.40497413  11.85944476  25.6612134\n",
      "   23.42695592]]\n",
      "step 6000, time 6.1, loss 1.30570635, epochs 8105, reward/epoch 9.35126\n",
      "[[ 23.25119885  18.603746    23.40549145   5.49891956  17.53127172\n",
      "   19.98022073  17.86655343  22.02225378  22.47633614  13.22665267\n",
      "   19.77626647   6.11439811   9.37957081  22.37253727  10.6297531\n",
      "   22.7163947 ]\n",
      " [ 24.39987173  19.88737299  24.5584393    6.71548643  18.77592823\n",
      "   21.26448405  19.1218267   23.15820968  23.56900422  14.46669934\n",
      "   21.05556203   7.34119725  10.56118802  23.46558107  11.8337382\n",
      "   23.80818491]]\n",
      "step 7000, time 7.1, loss 1.32126567, epochs 9816, reward/epoch 9.35126\n",
      "[[ -0.91659236  12.10120184  24.63422671  22.22380375  20.48779948\n",
      "   22.21750017   2.16516373  23.60860439  22.33686567  10.31365337\n",
      "    6.41422078  19.78736883  22.49012661  25.64942762   3.87529505\n",
      "    8.09815746]\n",
      " [  0.27391629  13.3133173   25.5950763   23.30486553  21.74166503\n",
      "   23.28335129   3.41866227  24.44629044  23.51755462  11.50990176\n",
      "    7.61990704  21.02458409  23.60754742  26.60570467   5.02685809\n",
      "    9.31593132]]\n",
      "step 8000, time 8.0, loss 1.31920909, epochs 11526, reward/epoch 9.35673\n",
      "[[ 19.38206483  18.67849129  21.02176504  10.06953915  18.00251028\n",
      "    6.18231426  22.7359335   17.92090458  21.98381214   7.87179628\n",
      "   22.83989625  20.80511965  23.87785049  23.04491713  10.62467377\n",
      "   10.9452347 ]\n",
      " [ 20.63720924  19.94019714  22.28308961  11.25459304  19.24468761\n",
      "    7.36901221  23.83966464  19.15438738  23.19497908   9.11032166\n",
      "   24.00448731  22.0547385   24.80205324  24.1548476   11.85578597\n",
      "   12.1690656 ]]\n",
      "step 9000, time 9.0, loss 1.30709839, epochs 13235, reward/epoch 9.36220\n",
      "[[ 22.80389926  22.6124855   24.62376723  22.00724372  22.62560963\n",
      "   22.99472676  19.83475511  11.30460001   2.40978778  22.79220467\n",
      "   22.41168406  11.95059591  23.65883789   8.80675787   7.00473864\n",
      "   19.52880678]\n",
      " [ 23.97952179  23.68353748  25.51922732  23.24952364  23.71732819\n",
      "   24.10306746  21.08114817  12.57460181   3.60891461  23.94245922\n",
      "   23.51051439  13.19916594  24.5529458   10.05331401   8.23071654\n",
      "   20.81212897]]\n",
      "step 10000, time 10.1, loss 1.34386292, epochs 14951, reward/epoch 9.32401\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "block_loss = 0.0\n",
    "last_epochs=0\n",
    "\n",
    "for istep in np.arange(nsteps): \n",
    "    if (render): envs[0].render()\n",
    "    epsilon = epsilons[istep] if istep < neps else epsilon_end\n",
    "    learning_r = learning_rates[istep] if istep < nlr else lr_end\n",
    "    update_estimator(target_estimator, q_estimator, target_window, istep)\n",
    "\n",
    "    A = policy(q_estimator, state, epsilon)\n",
    "    done_ind = []\n",
    "    for i, env in enumerate(envs):\n",
    "        actions[i] = np.random.choice(VALID_ACTIONS, p=A[:,i])\n",
    "        #actions[i] = np.random.choice(VALID_ACTIONS)\n",
    "        #print A[:,i], actions[i]\n",
    "        observation, reward, done, info = env.step(actions[i])\n",
    "        rewards[i] = reward\n",
    "        block_reward += reward\n",
    "        new_state[:,i] = np.concatenate((state[nfeats:,i], preprocess(observation)))\n",
    "        if done:\n",
    "            observation = envs[i].reset()\n",
    "            #new_state[:,i] = np.concatenate((preprocess(observation), preprocess(observation)))\n",
    "            #new_state[:,i] = np.concatenate((np.zeros_like(state[nfeats:,i]), preprocess(observation)))\n",
    "            total_epochs += 1\n",
    "            #rewards[i] = 0\n",
    "            done_ind.append(i)\n",
    "    Q_score = target_estimator.predict(new_state)\n",
    "    target = np.amax(Q_score, axis=0)\n",
    "    y = rewards + discount_factor*target\n",
    "    #print y[done_ind], rewards[done_ind]\n",
    "    y[done_ind] = rewards[done_ind]\n",
    "    loss = q_estimator.gradient(state, actions, y)\n",
    "    block_loss += loss\n",
    "    q_estimator.rmsprop(learning_r, decay_rate)\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "\n",
    "    #########################################################################\n",
    "    ## TODO: Implement Q-Learning                                          ##\n",
    "    ##                                                                     ##\n",
    "    ## At high level, your code should:                                    ##\n",
    "    ## * Update epsilon and learning rate.                                 ##\n",
    "    ## * Update target estimator from Q-estimator if needed.               ##\n",
    "    ## * Get the next action probabilities for the minibatch by running    ##\n",
    "    ##   the policy on the current state with the Q-estimator.             ##\n",
    "    ## * Then for each environment:                                        ##\n",
    "    ##     ** Pick an action according to the action probabilities.        ##\n",
    "    ##     ** Step in the gym with that action.                            ##\n",
    "    ##     ** Process the observation and concat it to the last nwindow-1  ##\n",
    "    ##        processed observations to form a new state.                  ##\n",
    "    ## Then for all environments (vectorized):                             ##\n",
    "    ## * Predict Q-scores for the new state using the target estimator.    ##\n",
    "    ## * Compute new expected rewards using those Q-scores.                ##\n",
    "    ## * Using those expected rewards as a target, compute gradients and   ##\n",
    "    ##   update the Q-estimator.                                           ##\n",
    "    ## * Step to the new state.                                            ##\n",
    "    ##                                                                     ##\n",
    "    #########################################################################\n",
    "\n",
    "    t = time.time() - t0\n",
    "    if (istep % printsteps == 0):\n",
    "        #print A\n",
    "        print q_estimator.predict(state)\n",
    "        #print state\n",
    "        print(\"step {:0d}, time {:.1f}, loss {:.8f}, epochs {:0d}, reward/epoch {:.5f}\".format(\n",
    "                istep, t, block_loss/printsteps, total_epochs, block_reward/np.maximum(1,total_epochs-last_epochs)))\n",
    "        last_epochs = total_epochs\n",
    "        block_reward = 0.0\n",
    "        block_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(q_estimator.model, open(\"cartpole_q_estimator.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reload the model later if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "test_estimator.model = pickle.load(open(\"cartpole_q_estimator.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And animate the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "GLXInfoException",
     "evalue": "pyglet requires an X server with GLX",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGLXInfoException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-8fbb3549f0c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstate0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0miaction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/gym-0.5.4-py2.7.egg/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/gym-0.5.4-py2.7.egg/gym/envs/classic_control/cartpole.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/gym-0.5.4-py2.7.egg/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/pyglet-1.2.4-py2.7.egg/pyglet/gl/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/pyglet-1.2.4-py2.7.egg/pyglet/window/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_epydoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/pyglet-1.2.4-py2.7.egg/pyglet/gl/__init__.pyc\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/pyglet-1.2.4-py2.7.egg/pyglet/window/xlib/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/pyglet-1.2.4-py2.7.egg/pyglet/window/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    503\u001b[0m                 None]:\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mNoSuchConfigException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/pyglet-1.2.4-py2.7.egg/pyglet/canvas/base.pyc\u001b[0m in \u001b[0;36mget_best_config\u001b[0;34m(self, template)\u001b[0m\n\u001b[1;32m    159\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoSuchConfigException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/pyglet-1.2.4-py2.7.egg/pyglet/canvas/xlib.pyc\u001b[0m in \u001b[0;36mget_matching_configs\u001b[0;34m(self, template)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_matching_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mcanvas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXlibCanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;31m# XXX deprecate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/pyglet-1.2.4-py2.7.egg/pyglet/gl/xlib.pyc\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, canvas)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglx_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLXInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mhave_13\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhave_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_13\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXlibCanvasConfig13\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunfanz/anaconda3/envs/pygen2/lib/python2.7/site-packages/pyglet-1.2.4-py2.7.egg/pyglet/gl/glx_info.pyc\u001b[0m in \u001b[0;36mhave_version\u001b[0;34m(self, major, minor)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mglXQueryExtension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGLXInfoException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pyglet requires an X server with GLX'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mserver_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_server_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGLXInfoException\u001b[0m: pyglet requires an X server with GLX"
     ]
    }
   ],
   "source": [
    "state0 = state[:,0]\n",
    "for i in np.arange(200):\n",
    "    envs[0].render()\n",
    "    preds = test_estimator.predict(state0)\n",
    "    iaction = np.argmax(preds)\n",
    "    obs, _, done0, _ = envs[0].step(VALID_ACTIONS[iaction])\n",
    "    state0 = np.concatenate((state0[nfeats:], preprocess(obs)))\n",
    "    if (done0): envs[0].reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there we have it. Simple 1-step Q-Learning can solve easy problems very fast. Note that environments that produce images will be much slower to train on than environments (like CartPole) which return an observation of the state of the system. But this model can still train on those image-based games - like Atari games. It will take hours-days however. It you try training on visual environments, we recommend you run the most expensive step - rmsprop - less often (e.g. every 10 iterations). This gives about a 3x speedup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional\n",
    "Do **one** of the following tasks:\n",
    "* Adapt the DQN algorithm to another environment - it can use direct state observations.  Call <code>env.get_action_meanings()</code> to find out what actions are allowed. Summarize training performance: your final average reward/epoch, the number of steps required to train, and any modifications to the model or its parameters that you made.\n",
    "* Try smarter schedules for epsilon and learning rate. Rewards for CartPole increase very sharply (several orders of magnitude) with better policies, especially as epsilon --> 0. Gradients will also change drastically, so the initial learning rate is probably not good later on. Try schedules for decreasing epsilon that allow the model to better adapt. Try other learning rate schedules, or setting learning rate based on average reward. \n",
    "* Try a fancier model. e.g. add another hidden layer, or try sigmoid non-linearities."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pygen2]",
   "language": "python",
   "name": "conda-env-pygen2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
